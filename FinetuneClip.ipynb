{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO0NV3mnWKsSrUm6o2HbN5K"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "#solution for importerror when running transforms repo scripts:\n",
        "# https://discuss.huggingface.co/t/getting-error-while-fine-tuning-deberta-v3-large/11486 | https://colab.research.google.com/drive/1K_FGbcPG1hPltM-BYcx3IB10T-Hp2Lzy?usp=sharing#scrollTo=-FA1IjqHLADQ\n",
        "!rm -r transformers\n",
        "!git clone https://github.com/huggingface/transformers.git\n",
        "!cd transformers\n",
        "!pip install -q ./transformers"
      ],
      "metadata": {
        "id": "U3e0IhmYy2o0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRh1Ph1DN7v9",
        "outputId": "872fb7d1-3092-43e2-ccd1-55d5e802b892"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "import os\n",
        "from PIL import ImageFile\n",
        "import json\n",
        "import pathlib\n",
        "from typing import Generator\n",
        "import datasets\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7JIOc2rOK9O",
        "outputId": "610eda63-a53e-4860-afe7-cb395421fc62"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "source repo for finetuning vlms: https://github.com/damian0815/finetune-clip-huggingface/tree/main"
      ],
      "metadata": {
        "id": "7QeMKrX80uJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collect_captioned_images(root_folder: str) -> Generator[tuple[str,str], None, None]:\n",
        "    image_paths = []\n",
        "    captions = []\n",
        "\n",
        "    for directory, _, filenames in os.walk(root_folder):\n",
        "        image_extensions = ['.jpg', '.jpeg']\n",
        "        image_filenames = [f for f in filenames if os.path.splitext(f)[1] in image_extensions]\n",
        "        for image_filename in image_filenames:\n",
        "            caption_filename = os.path.splitext(image_filename)[0] + '.txt'\n",
        "            caption_path = os.path.join(directory, caption_filename)\n",
        "            if not os.path.exists(caption_path):\n",
        "                continue\n",
        "\n",
        "            with open(caption_path, 'r') as f:\n",
        "                caption = f.read().replace('\\n', ' ')\n",
        "\n",
        "                image_path = os.path.join(directory, image_filename)\n",
        "                yield image_path, caption\n",
        "\n",
        "\n",
        "def convert_text_image_pairs_to_huggingface_json(root_folder, out_json):\n",
        "    out_folder = os.path.dirname(root_folder)\n",
        "    pathlib.Path(out_folder).mkdir(parents=True, exist_ok=True)\n",
        "    with open(out_json, \"w\") as f:\n",
        "        written_count = 0\n",
        "        for image_path, caption in collect_captioned_images(root_folder):\n",
        "            line_dict = {\"image\":image_path, \"caption\":caption}\n",
        "            json_line = json.dumps(line_dict, indent=None, separators=(\",\",\":\"))\n",
        "            #print(json_line)\n",
        "            f.write(json_line + \"\\n\")\n",
        "            written_count += 1\n",
        "        print(f\"wrote {written_count} lines to {out_json}\")"
      ],
      "metadata": {
        "id": "To_Y_GwqOZDn"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "image-txt caption pair to HF compatiable json format"
      ],
      "metadata": {
        "id": "W7XsPuO_vGwh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#convert image-caption pair from img-txt format to json format with image pathing\n",
        "root_folder = '/content/drive/MyDrive/image-search-project/carBrandsSmallImgTextPairs'\n",
        "out_json = '/content/drive/MyDrive/image-search-project/carBrandsSmallImgTextPairs.json'\n",
        "convert_text_image_pairs_to_huggingface_json(root_folder, out_json)"
      ],
      "metadata": {
        "id": "aqIs_u3TkRzP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfec3e4f-d85c-44e7-ea28-11b4069ede95"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wrote 93 lines to /content/drive/MyDrive/image-search-project/carBrandsSmallImgTextPairs.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "pull HF's transformers repo, finetuning code is under examples/pytorch/contrastive-image-txt/run_clip.py: https://github.com/huggingface/transformers/tree/main/examples/pytorch/contrastive-image-text"
      ],
      "metadata": {
        "id": "10KF2uhxvnR1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#fine tuning params:\n",
        "repo_id =  \"openai/clip-vit-base-patch32\"\n",
        "output_folder = \"/content/drive/MyDrive/image-search-project/clip-finetune-results\"\n",
        "batch_size = 16\n",
        "num_train_epochs =50"
      ],
      "metadata": {
        "id": "4kSfmAidvXPH"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Finetuning {repo_id} for {num_train_epochs} epochs with batch size {batch_size}, and then saving output to {output_folder}.\")\n",
        "!python /content/transformers/examples/pytorch/contrastive-image-text/run_clip.py \\\n",
        "    --output_dir {output_folder} \\\n",
        "    --model_name_or_path {repo_id} \\\n",
        "    --train_file {out_json} \\\n",
        "    --image_column image \\\n",
        "    --overwrite_output_dir=True \\\n",
        "    --max_seq_length=77 \\\n",
        "    --num_train_epochs={num_train_epochs} \\\n",
        "    --remove_unused_columns=False \\\n",
        "    --do_train \\\n",
        "    --per_device_train_batch_size={batch_size} \\\n",
        "    --learning_rate=\"5e-5\" --warmup_steps=\"0\" --weight_decay 0.1\n",
        "print(\"--\\nDONE\")\n",
        "print(f\"If it worked, trained data should be in {output_folder}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qm37L4NiwsRt",
        "outputId": "6a62d748-ffe3-4b5a-f8fd-cb2671e2130c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finetuning openai/clip-vit-base-patch32 for 50 epochs with batch size 16, and then saving output to /content/drive/MyDrive/image-search-project/clip-finetune-results.\n",
            "2025-05-02 07:06:56.848205: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1746169616.881896   10637 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1746169616.892542   10637 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "05/02/2025 07:07:01 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "05/02/2025 07:07:01 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "average_tokens_across_devices=False,\n",
            "batch_eval_metrics=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_on_start=False,\n",
            "eval_steps=None,\n",
            "eval_strategy=IntervalStrategy.NO,\n",
            "eval_use_gather_object=False,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_for_metrics=[],\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/drive/MyDrive/image-search-project/clip-finetune-results/runs/May02_07-07-01_0641f0190c84,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=50.0,\n",
            "optim=OptimizerNames.ADAMW_TORCH,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=/content/drive/MyDrive/image-search-project/clip-finetune-results,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=False,\n",
            "report_to=['tensorboard', 'wandb'],\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/drive/MyDrive/image-search-project/clip-finetune-results,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=SaveStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torch_empty_cache_steps=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_liger_kernel=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.1,\n",
            ")\n",
            "[INFO|configuration_utils.py:694] 2025-05-02 07:07:02,221 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--openai--clip-vit-base-patch32/snapshots/3d74acf9a28c67741b2f4f2ea7635f0aaf6f0268/config.json\n",
            "[INFO|configuration_clip.py:350] 2025-05-02 07:07:02,227 >> `text_config` is `None`. Initializing the `CLIPTextConfig` with default values.\n",
            "[INFO|configuration_clip.py:354] 2025-05-02 07:07:02,227 >> `vision_config` is `None`. initializing the `CLIPVisionConfig` with default values.\n",
            "[INFO|configuration_utils.py:766] 2025-05-02 07:07:02,228 >> Model config CLIPConfig {\n",
            "  \"architectures\": [\n",
            "    \"CLIPModel\"\n",
            "  ],\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"logit_scale_init_value\": 2.6592,\n",
            "  \"model_type\": \"clip\",\n",
            "  \"projection_dim\": 512,\n",
            "  \"text_config\": {\n",
            "    \"attention_dropout\": 0.0,\n",
            "    \"bos_token_id\": 0,\n",
            "    \"dropout\": 0.0,\n",
            "    \"eos_token_id\": 2,\n",
            "    \"hidden_act\": \"quick_gelu\",\n",
            "    \"hidden_size\": 512,\n",
            "    \"initializer_factor\": 1.0,\n",
            "    \"initializer_range\": 0.02,\n",
            "    \"intermediate_size\": 2048,\n",
            "    \"layer_norm_eps\": 1e-05,\n",
            "    \"max_position_embeddings\": 77,\n",
            "    \"model_type\": \"clip_text_model\",\n",
            "    \"num_attention_heads\": 8,\n",
            "    \"num_hidden_layers\": 12,\n",
            "    \"projection_dim\": 512,\n",
            "    \"vocab_size\": 49408\n",
            "  },\n",
            "  \"transformers_version\": \"4.52.0.dev0\",\n",
            "  \"vision_config\": {\n",
            "    \"attention_dropout\": 0.0,\n",
            "    \"dropout\": 0.0,\n",
            "    \"hidden_act\": \"quick_gelu\",\n",
            "    \"hidden_size\": 768,\n",
            "    \"image_size\": 224,\n",
            "    \"initializer_factor\": 1.0,\n",
            "    \"initializer_range\": 0.02,\n",
            "    \"intermediate_size\": 3072,\n",
            "    \"layer_norm_eps\": 1e-05,\n",
            "    \"model_type\": \"clip_vision_model\",\n",
            "    \"num_attention_heads\": 12,\n",
            "    \"num_channels\": 3,\n",
            "    \"num_hidden_layers\": 12,\n",
            "    \"patch_size\": 32,\n",
            "    \"projection_dim\": 512\n",
            "  }\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2096] 2025-05-02 07:07:02,335 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--openai--clip-vit-base-patch32/snapshots/3d74acf9a28c67741b2f4f2ea7635f0aaf6f0268/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2096] 2025-05-02 07:07:02,335 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--openai--clip-vit-base-patch32/snapshots/3d74acf9a28c67741b2f4f2ea7635f0aaf6f0268/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2096] 2025-05-02 07:07:02,335 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--openai--clip-vit-base-patch32/snapshots/3d74acf9a28c67741b2f4f2ea7635f0aaf6f0268/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2096] 2025-05-02 07:07:02,335 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2096] 2025-05-02 07:07:02,335 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--openai--clip-vit-base-patch32/snapshots/3d74acf9a28c67741b2f4f2ea7635f0aaf6f0268/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2096] 2025-05-02 07:07:02,335 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--openai--clip-vit-base-patch32/snapshots/3d74acf9a28c67741b2f4f2ea7635f0aaf6f0268/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2096] 2025-05-02 07:07:02,335 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|configuration_utils.py:694] 2025-05-02 07:07:02,336 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--openai--clip-vit-base-patch32/snapshots/3d74acf9a28c67741b2f4f2ea7635f0aaf6f0268/config.json\n",
            "[INFO|configuration_clip.py:350] 2025-05-02 07:07:02,336 >> `text_config` is `None`. Initializing the `CLIPTextConfig` with default values.\n",
            "[INFO|configuration_clip.py:354] 2025-05-02 07:07:02,337 >> `vision_config` is `None`. initializing the `CLIPVisionConfig` with default values.\n",
            "[INFO|configuration_utils.py:766] 2025-05-02 07:07:02,338 >> Model config CLIPConfig {\n",
            "  \"architectures\": [\n",
            "    \"CLIPModel\"\n",
            "  ],\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"logit_scale_init_value\": 2.6592,\n",
            "  \"model_type\": \"clip\",\n",
            "  \"projection_dim\": 512,\n",
            "  \"text_config\": {\n",
            "    \"attention_dropout\": 0.0,\n",
            "    \"bos_token_id\": 0,\n",
            "    \"dropout\": 0.0,\n",
            "    \"eos_token_id\": 2,\n",
            "    \"hidden_act\": \"quick_gelu\",\n",
            "    \"hidden_size\": 512,\n",
            "    \"initializer_factor\": 1.0,\n",
            "    \"initializer_range\": 0.02,\n",
            "    \"intermediate_size\": 2048,\n",
            "    \"layer_norm_eps\": 1e-05,\n",
            "    \"max_position_embeddings\": 77,\n",
            "    \"model_type\": \"clip_text_model\",\n",
            "    \"num_attention_heads\": 8,\n",
            "    \"num_hidden_layers\": 12,\n",
            "    \"projection_dim\": 512,\n",
            "    \"vocab_size\": 49408\n",
            "  },\n",
            "  \"transformers_version\": \"4.52.0.dev0\",\n",
            "  \"vision_config\": {\n",
            "    \"attention_dropout\": 0.0,\n",
            "    \"dropout\": 0.0,\n",
            "    \"hidden_act\": \"quick_gelu\",\n",
            "    \"hidden_size\": 768,\n",
            "    \"image_size\": 224,\n",
            "    \"initializer_factor\": 1.0,\n",
            "    \"initializer_range\": 0.02,\n",
            "    \"intermediate_size\": 3072,\n",
            "    \"layer_norm_eps\": 1e-05,\n",
            "    \"model_type\": \"clip_vision_model\",\n",
            "    \"num_attention_heads\": 12,\n",
            "    \"num_channels\": 3,\n",
            "    \"num_hidden_layers\": 12,\n",
            "    \"patch_size\": 32,\n",
            "    \"projection_dim\": 512\n",
            "  }\n",
            "}\n",
            "\n",
            "[INFO|image_processing_base.py:380] 2025-05-02 07:07:02,552 >> loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--openai--clip-vit-base-patch32/snapshots/3d74acf9a28c67741b2f4f2ea7635f0aaf6f0268/preprocessor_config.json\n",
            "[WARNING|logging.py:328] 2025-05-02 07:07:02,552 >> Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "[INFO|image_processing_utils.py:241] 2025-05-02 07:07:02,553 >> size should be a dictionary on of the following set of keys: ({'width', 'height'}, {'shortest_edge'}, {'shortest_edge', 'longest_edge'}, {'longest_edge'}, {'max_height', 'max_width'}), got 224. Converted to {'shortest_edge': 224}.\n",
            "[INFO|image_processing_utils.py:241] 2025-05-02 07:07:02,553 >> crop_size should be a dictionary on of the following set of keys: ({'width', 'height'}, {'shortest_edge'}, {'shortest_edge', 'longest_edge'}, {'longest_edge'}, {'max_height', 'max_width'}), got 224. Converted to {'height': 224, 'width': 224}.\n",
            "[INFO|image_processing_base.py:433] 2025-05-02 07:07:02,553 >> Image processor CLIPImageProcessor {\n",
            "  \"crop_size\": {\n",
            "    \"height\": 224,\n",
            "    \"width\": 224\n",
            "  },\n",
            "  \"do_center_crop\": true,\n",
            "  \"do_convert_rgb\": true,\n",
            "  \"do_normalize\": true,\n",
            "  \"do_rescale\": true,\n",
            "  \"do_resize\": true,\n",
            "  \"image_mean\": [\n",
            "    0.48145466,\n",
            "    0.4578275,\n",
            "    0.40821073\n",
            "  ],\n",
            "  \"image_processor_type\": \"CLIPImageProcessor\",\n",
            "  \"image_std\": [\n",
            "    0.26862954,\n",
            "    0.26130258,\n",
            "    0.27577711\n",
            "  ],\n",
            "  \"resample\": 3,\n",
            "  \"rescale_factor\": 0.00392156862745098,\n",
            "  \"size\": {\n",
            "    \"shortest_edge\": 224\n",
            "  }\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:694] 2025-05-02 07:07:02,635 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--openai--clip-vit-base-patch32/snapshots/3d74acf9a28c67741b2f4f2ea7635f0aaf6f0268/config.json\n",
            "[INFO|configuration_clip.py:350] 2025-05-02 07:07:02,636 >> `text_config` is `None`. Initializing the `CLIPTextConfig` with default values.\n",
            "[INFO|configuration_clip.py:354] 2025-05-02 07:07:02,636 >> `vision_config` is `None`. initializing the `CLIPVisionConfig` with default values.\n",
            "[INFO|configuration_utils.py:766] 2025-05-02 07:07:02,638 >> Model config CLIPConfig {\n",
            "  \"architectures\": [\n",
            "    \"CLIPModel\"\n",
            "  ],\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"logit_scale_init_value\": 2.6592,\n",
            "  \"model_type\": \"clip\",\n",
            "  \"projection_dim\": 512,\n",
            "  \"text_config\": {\n",
            "    \"attention_dropout\": 0.0,\n",
            "    \"bos_token_id\": 0,\n",
            "    \"dropout\": 0.0,\n",
            "    \"eos_token_id\": 2,\n",
            "    \"hidden_act\": \"quick_gelu\",\n",
            "    \"hidden_size\": 512,\n",
            "    \"initializer_factor\": 1.0,\n",
            "    \"initializer_range\": 0.02,\n",
            "    \"intermediate_size\": 2048,\n",
            "    \"layer_norm_eps\": 1e-05,\n",
            "    \"max_position_embeddings\": 77,\n",
            "    \"model_type\": \"clip_text_model\",\n",
            "    \"num_attention_heads\": 8,\n",
            "    \"num_hidden_layers\": 12,\n",
            "    \"projection_dim\": 512,\n",
            "    \"vocab_size\": 49408\n",
            "  },\n",
            "  \"transformers_version\": \"4.52.0.dev0\",\n",
            "  \"vision_config\": {\n",
            "    \"attention_dropout\": 0.0,\n",
            "    \"dropout\": 0.0,\n",
            "    \"hidden_act\": \"quick_gelu\",\n",
            "    \"hidden_size\": 768,\n",
            "    \"image_size\": 224,\n",
            "    \"initializer_factor\": 1.0,\n",
            "    \"initializer_range\": 0.02,\n",
            "    \"intermediate_size\": 3072,\n",
            "    \"layer_norm_eps\": 1e-05,\n",
            "    \"model_type\": \"clip_vision_model\",\n",
            "    \"num_attention_heads\": 12,\n",
            "    \"num_channels\": 3,\n",
            "    \"num_hidden_layers\": 12,\n",
            "    \"patch_size\": 32,\n",
            "    \"projection_dim\": 512\n",
            "  }\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:1113] 2025-05-02 07:07:05,209 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--openai--clip-vit-base-patch32/snapshots/3d74acf9a28c67741b2f4f2ea7635f0aaf6f0268/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:2188] 2025-05-02 07:07:05,210 >> Instantiating CLIPTextModel model under default dtype torch.float32.\n",
            "[INFO|modeling_utils.py:2188] 2025-05-02 07:07:05,221 >> Instantiating CLIPVisionModel model under default dtype torch.float32.\n",
            "[INFO|safetensors_conversion.py:61] 2025-05-02 07:07:05,315 >> Attempting to create safetensors variant\n",
            "[INFO|safetensors_conversion.py:74] 2025-05-02 07:07:05,693 >> Safetensors PR exists\n",
            "[INFO|modeling_utils.py:5053] 2025-05-02 07:07:05,746 >> All model checkpoint weights were used when initializing CLIPModel.\n",
            "\n",
            "[INFO|modeling_utils.py:5061] 2025-05-02 07:07:05,746 >> All the weights of CLIPModel were initialized from the model checkpoint at openai/clip-vit-base-patch32.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use CLIPModel for predictions without further training.\n",
            "Parameter 'transform'=<function main.<locals>.transform_images at 0x7afa42ec79c0> of the transform datasets.arrow_dataset.Dataset.set_format couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "05/02/2025 07:07:06 - WARNING - datasets.fingerprint - Parameter 'transform'=<function main.<locals>.transform_images at 0x7afa42ec79c0> of the transform datasets.arrow_dataset.Dataset.set_format couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "[INFO|trainer.py:2408] 2025-05-02 07:07:06,845 >> ***** Running training *****\n",
            "[INFO|trainer.py:2409] 2025-05-02 07:07:06,845 >>   Num examples = 93\n",
            "[INFO|trainer.py:2410] 2025-05-02 07:07:06,845 >>   Num Epochs = 50\n",
            "[INFO|trainer.py:2411] 2025-05-02 07:07:06,846 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:2414] 2025-05-02 07:07:06,846 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:2415] 2025-05-02 07:07:06,846 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:2416] 2025-05-02 07:07:06,846 >>   Total optimization steps = 300\n",
            "[INFO|trainer.py:2417] 2025-05-02 07:07:06,847 >>   Number of trainable parameters = 151,277,313\n",
            "[INFO|configuration_clip.py:350] 2025-05-02 07:07:06,860 >> `text_config` is `None`. Initializing the `CLIPTextConfig` with default values.\n",
            "[INFO|configuration_clip.py:354] 2025-05-02 07:07:06,861 >> `vision_config` is `None`. initializing the `CLIPVisionConfig` with default values.\n",
            "[INFO|integration_utils.py:831] 2025-05-02 07:07:06,862 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose \"Don't visualize my results\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B syncing is set to \u001b[1m`offline`\u001b[0m in this directory. Run \u001b[1m`wandb online`\u001b[0m or set \u001b[1mWANDB_MODE=online\u001b[0m to enable cloud syncing.\n",
            "100% 300/300 [04:03<00:00,  1.63it/s][INFO|trainer.py:3992] 2025-05-02 07:12:36,003 >> Saving model checkpoint to /content/drive/MyDrive/image-search-project/clip-finetune-results/checkpoint-300\n",
            "[INFO|configuration_clip.py:350] 2025-05-02 07:12:36,005 >> `text_config` is `None`. Initializing the `CLIPTextConfig` with default values.\n",
            "[INFO|configuration_clip.py:354] 2025-05-02 07:12:36,005 >> `vision_config` is `None`. initializing the `CLIPVisionConfig` with default values.\n",
            "[INFO|configuration_clip.py:350] 2025-05-02 07:12:36,006 >> `text_config` is `None`. Initializing the `CLIPTextConfig` with default values.\n",
            "[INFO|configuration_clip.py:354] 2025-05-02 07:12:36,006 >> `vision_config` is `None`. initializing the `CLIPVisionConfig` with default values.\n",
            "[INFO|configuration_clip.py:350] 2025-05-02 07:12:36,010 >> `text_config` is `None`. Initializing the `CLIPTextConfig` with default values.\n",
            "[INFO|configuration_clip.py:354] 2025-05-02 07:12:36,010 >> `vision_config` is `None`. initializing the `CLIPVisionConfig` with default values.\n",
            "[INFO|configuration_utils.py:420] 2025-05-02 07:12:36,013 >> Configuration saved in /content/drive/MyDrive/image-search-project/clip-finetune-results/checkpoint-300/config.json\n",
            "[INFO|modeling_utils.py:3642] 2025-05-02 07:12:40,213 >> Model weights saved in /content/drive/MyDrive/image-search-project/clip-finetune-results/checkpoint-300/model.safetensors\n",
            "[INFO|trainer.py:2675] 2025-05-02 07:12:48,164 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 341.3179, 'train_samples_per_second': 13.624, 'train_steps_per_second': 0.879, 'train_loss': 0.5786690775553386, 'epoch': 50.0}\n",
            "100% 300/300 [04:16<00:00,  1.17it/s]\n",
            "[INFO|trainer.py:3992] 2025-05-02 07:12:48,173 >> Saving model checkpoint to /content/drive/MyDrive/image-search-project/clip-finetune-results\n",
            "[INFO|configuration_clip.py:350] 2025-05-02 07:12:48,175 >> `text_config` is `None`. Initializing the `CLIPTextConfig` with default values.\n",
            "[INFO|configuration_clip.py:354] 2025-05-02 07:12:48,175 >> `vision_config` is `None`. initializing the `CLIPVisionConfig` with default values.\n",
            "[INFO|configuration_clip.py:350] 2025-05-02 07:12:48,176 >> `text_config` is `None`. Initializing the `CLIPTextConfig` with default values.\n",
            "[INFO|configuration_clip.py:354] 2025-05-02 07:12:48,176 >> `vision_config` is `None`. initializing the `CLIPVisionConfig` with default values.\n",
            "[INFO|configuration_clip.py:350] 2025-05-02 07:12:48,182 >> `text_config` is `None`. Initializing the `CLIPTextConfig` with default values.\n",
            "[INFO|configuration_clip.py:354] 2025-05-02 07:12:48,182 >> `vision_config` is `None`. initializing the `CLIPVisionConfig` with default values.\n",
            "[INFO|configuration_utils.py:420] 2025-05-02 07:12:48,186 >> Configuration saved in /content/drive/MyDrive/image-search-project/clip-finetune-results/config.json\n",
            "[INFO|modeling_utils.py:3642] 2025-05-02 07:12:51,905 >> Model weights saved in /content/drive/MyDrive/image-search-project/clip-finetune-results/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2598] 2025-05-02 07:12:52,392 >> tokenizer config file saved in /content/drive/MyDrive/image-search-project/clip-finetune-results/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2607] 2025-05-02 07:12:52,400 >> Special tokens file saved in /content/drive/MyDrive/image-search-project/clip-finetune-results/special_tokens_map.json\n",
            "[INFO|image_processing_base.py:260] 2025-05-02 07:12:52,493 >> Image processor saved in /content/drive/MyDrive/image-search-project/clip-finetune-results/preprocessor_config.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       50.0\n",
            "  total_flos               =   251900GF\n",
            "  train_loss               =     0.5787\n",
            "  train_runtime            = 0:05:41.31\n",
            "  train_samples_per_second =     13.624\n",
            "  train_steps_per_second   =      0.879\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: You can sync this run to the cloud by running:\n",
            "\u001b[1;34mwandb\u001b[0m: \u001b[1mwandb sync /content/wandb/offline-run-20250502_070831-5dlhg8xr\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/offline-run-20250502_070831-5dlhg8xr/logs\u001b[0m\n",
            "--\n",
            "DONE\n",
            "If it worked, trained data should be in /content/drive/MyDrive/image-search-project/clip-finetune-results\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#testing finetuned clip model, load the output_dir\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "clip_model = CLIPModel.from_pretrained(\"/content/drive/MyDrive/image-search-project/clip-finetune-results\").to(device)"
      ],
      "metadata": {
        "id": "eVyFMum4z4af"
      },
      "execution_count": 41,
      "outputs": []
    }
  ]
}